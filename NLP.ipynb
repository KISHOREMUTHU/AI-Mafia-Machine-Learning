{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "identical-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data collection\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consolidated-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recent-adams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "2997\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories = 'editorial')\n",
    "print(data)\n",
    "print(type(data) )\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "balanced-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenisation and stopword removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polished-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "text = \"A very pleasant day and it's cool to go for a jog .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "likely-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A very pleasant day and it's cool to go for a jog .\"]\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "engaged-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'very', 'pleasant', 'day', 'and', 'it', \"'s\", 'cool', 'to', 'go', 'for', 'a', 'jog', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aerial-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords removal\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "induced-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'having', 'here', 'through', 'while', 'or', 'just', \"you'd\", 'they', 'can', \"won't\", 'isn', 'was', 'he', 'nor', 'too', 'whom', 'before', 'how', 'we', 've', 'ma', 'will', 'after', \"isn't\", 'very', 'don', 'such', 'further', 'so', \"doesn't\", 'then', 'their', 'on', 'll', 'been', 'this', 'between', 'down', 'most', \"you'll\", 'only', 'mightn', 'few', 'weren', 'theirs', \"it's\", 'themselves', \"wasn't\", 'd', 'your', 'any', 'won', 'during', 'needn', 'in', 'yourself', 'm', 'below', 'out', 'against', 'herself', 'for', 'do', 'it', 'off', 'over', 'there', 'yourselves', \"she's\", 'did', 'couldn', 'if', 'again', 'wasn', 'about', 'being', 'ain', 'has', 'of', 'doing', 'some', \"shouldn't\", 'but', 'into', 'wouldn', \"needn't\", 'shan', 'than', 'now', 'shouldn', 'were', 'y', 'all', 'more', \"didn't\", \"hadn't\", 'o', 'didn', 's', 'as', 'have', 't', 'doesn', 'she', \"don't\", 'once', 'those', 'hers', 'itself', 'both', \"should've\", 'mustn', 'our', 'ours', 'myself', 'himself', 'does', 'had', 'same', 'hasn', 'a', \"that'll\", 'own', 'are', 're', 'which', 'her', \"mustn't\", 'until', 'me', 'an', 'the', 'why', 'not', \"shan't\", 'is', 'what', 'by', 'no', 'where', 'who', 'aren', 'should', 'haven', 'to', 'up', 'i', \"you're\", \"couldn't\", 'with', 'you', 'that', \"weren't\", 'him', 'yours', \"wouldn't\", 'other', 'when', 'my', \"you've\", 'them', 'be', \"aren't\", \"mightn't\", 'its', 'under', 'at', 'these', 'each', 'ourselves', \"haven't\", 'hadn', 'and', \"hasn't\", 'his', 'because', 'am', 'from', 'above'} 179\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "print(sw,len(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unexpected-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the stopwords \n",
    "def filter_words(word_list) :\n",
    "    sw = set(stopwords.words('english'))\n",
    "    useful_words = [w for w in word_list if w  not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "worse-malta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', \"'s\", 'cool', 'go', 'jog', '.']\n"
     ]
    }
   ],
   "source": [
    "useful = filter_words(word_list)\n",
    "print(useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interior-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "removed-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'out', 'the', 'alphabets']\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(\"[a-z0-9]+\")\n",
    "sent = \"find out the alphabets\"\n",
    "print(token.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-thousand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-halloween",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "individual-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lazy', 'dog', 'from', 'high', 'wall', 'foxes', 'love', 'to', 'make', 'jumps']\n"
     ]
    }
   ],
   "source": [
    "text = 'the quick brown fox was seen jumping over the lazy dog from high wall . Foxes love to make jumps .'\n",
    "print(token.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fabulous-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of stemmers \n",
    "# 1 . Snowball stemmer \n",
    "# 2 . Porter stemmer \n",
    "# 3 . Lancaster stemmer\n",
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "linear-insurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "ps.stem('jumping') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pregnant-reunion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awe'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('aweful') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "corporate-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesom'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls =LancasterStemmer()\n",
    "ls.stem('awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "plastic-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teen\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "print(ls.stem('teenager'))\n",
    "print(ps.stem('teenager'))\n",
    "# different types of stemming employs different types of algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "young-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teenag'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=SnowballStemmer('english')\n",
    "ss.stem('teenager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "minute-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "private-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Indian cricket team will win the world cup , says Virat Kohli , since world cup happens in India\",\n",
    "           \"We will win the election this year , says Stalin\",\n",
    "          \"Rabindranath Tagore won the hearts of people\" ,\n",
    "          \"Raasi is an exciting thriller movie\",\n",
    "         ],\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hawaiian-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Indian cricket team will win the world cup , says Virat Kohli , since world cup happens in India', 'We will win the election this year , says Stalin', 'Rabindranath Tagore won the hearts of people', 'Raasi is an exciting thriller movie'],)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "excess-companion",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-fcf9591ed575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# print result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyTokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-fcf9591ed575>\u001b[0m in \u001b[0;36mmyTokenize\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Tokenizing strings in list of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmyTokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfilter_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python39\\lib\\tokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(readline)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mtells\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \"\"\"\n\u001b[1;32m--> 423\u001b[1;33m     \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m     \u001b[0mempty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_itertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[0mrl_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_itertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python39\\lib\\tokenize.py\u001b[0m in \u001b[0;36mdetect_encoding\u001b[1;34m(readline)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBOM_UTF8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mbom_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python39\\lib\\tokenize.py\u001b[0m in \u001b[0;36mread_or_stop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# convert word into numerical features\n",
    "# build common vocabulary and to vectorise the document \n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = [\"Indian cricket team will win the world cup ,\" ,\"says Virat Kohli , since world cup happens in India\"]\n",
    "   \n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "# using map() + split()\n",
    "# Tokenizing strings in list of strings\n",
    "def myTokenize(sentence):\n",
    "    words = list(tokenize.tokenize(sentence.lower()))\n",
    "    return filter_words(words)\n",
    "    \n",
    "  \n",
    "# print result\n",
    "print(myTokenize(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(filtered)\n",
    "bag = []\n",
    "for i in range (len(filtered)):\n",
    "      bag.append(filtered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-silicon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-symposium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
