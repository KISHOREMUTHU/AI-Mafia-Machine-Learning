{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "identical-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data collection\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consolidated-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recent-adams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.'], ...]\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "2997\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories = 'editorial')\n",
    "print(data)\n",
    "print(type(data) )\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "balanced-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenisation and stopword removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polished-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "text = \"A very pleasant day and it's cool to go for a jog .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "likely-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A very pleasant day and it's cool to go for a jog .\"]\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "engaged-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'very', 'pleasant', 'day', 'and', 'it', \"'s\", 'cool', 'to', 'go', 'for', 'a', 'jog', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aerial-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords removal\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "induced-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be', 'o', 'y', 'until', 'their', 'not', 'hadn', 'hasn', 'through', 'from', 'but', \"isn't\", 'i', \"you've\", 'doing', 'how', 'each', 'ours', 'shouldn', 'over', 'my', 'than', 'a', 'if', 'at', 'other', 'won', \"weren't\", 'where', \"hadn't\", \"you're\", 'below', 'the', 'as', 'about', 'of', 'him', 'nor', 'under', 'out', 'some', 'hers', \"you'd\", 'these', 'now', 'yourself', 'myself', 'into', 'having', 'by', 'were', 'me', 'had', \"needn't\", 'is', 'can', 'those', \"wouldn't\", \"it's\", 'why', 'themselves', 'isn', \"shan't\", 'yourselves', 'what', \"wasn't\", 'whom', 'few', 'very', 'our', 'am', 'further', 've', 'should', 'wouldn', 'been', \"haven't\", 'don', 'needn', 'who', \"didn't\", 'most', \"mightn't\", 'theirs', 'same', 'aren', 'ma', 'no', 'mustn', 'once', 'for', \"aren't\", 'down', 'himself', 'mightn', 'in', 'more', 'your', 'couldn', \"shouldn't\", 'she', \"mustn't\", \"won't\", 'we', 'does', 'an', 'he', 'before', 'both', 'it', 't', \"you'll\", 'll', 'ain', 'only', 'that', 'own', 'you', \"should've\", 'with', 'on', 'when', 'while', 'which', 'them', 's', 're', 'off', 'such', 'too', \"couldn't\", \"she's\", 'shan', 'doesn', 'her', 'between', 'haven', \"doesn't\", 'do', \"that'll\", 'ourselves', 'to', 'did', 'because', 'there', 'didn', 'will', 'up', 'they', 'being', 'has', 'here', 'against', 'then', 'so', 'was', 'd', 'any', 'wasn', 'above', 'during', 'again', 'are', 'after', 'or', 'yours', 'herself', 'all', \"hasn't\", 'this', 'weren', 'm', 'have', 'just', \"don't\", 'itself', 'its', 'and', 'his'} 179\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "print(sw,len(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unexpected-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the stopwords \n",
    "def filter_words(word_list) :\n",
    "    sw = set(stopwords.words('english'))\n",
    "    useful_words = [w for w in word_list if w  not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "worse-malta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', \"'s\", 'cool', 'go', 'jog', '.']\n"
     ]
    }
   ],
   "source": [
    "useful = filter_words(word_list)\n",
    "print(useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interior-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "removed-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'out', 'the', 'alphabets']\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(\"[a-z0-9]+\")\n",
    "sent = \"find out the alphabets\"\n",
    "print(token.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-thousand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-halloween",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "individual-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lazy', 'dog', 'from', 'high', 'wall', 'foxes', 'love', 'to', 'make', 'jumps']\n"
     ]
    }
   ],
   "source": [
    "text = 'the quick brown fox was seen jumping over the lazy dog from high wall . Foxes love to make jumps .'\n",
    "print(token.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fabulous-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of stemmers \n",
    "# 1 . Snowball stemmer \n",
    "# 2 . Porter stemmer \n",
    "# 3 . Lancaster stemmer\n",
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "linear-insurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "ps.stem('jumping') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pregnant-reunion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awe'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('aweful') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "corporate-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesom'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls =LancasterStemmer()\n",
    "ls.stem('awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "plastic-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teen\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "print(ls.stem('teenager'))\n",
    "print(ps.stem('teenager'))\n",
    "# different types of stemming employs different types of algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "young-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teenag'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=SnowballStemmer('english')\n",
    "ss.stem('teenager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "minute-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "private-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Indian cricket team will win the world cup , says Virat Kohli , since world cup happens in India\",\n",
    "           \"We will win the election this year , says Stalin\",\n",
    "          \"Rabindranath Tagore won the hearts of people\" ,\n",
    "          \"Raasi is an exciting thriller movie\",\n",
    "         ],\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hawaiian-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Indian cricket team will win the world cup , says Virat Kohli , since world cup happens in India', 'We will win the election this year , says Stalin', 'Rabindranath Tagore won the hearts of people', 'Raasi is an exciting thriller movie'],)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "excess-companion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list after split of strings is : [['Indian', 'cricket', 'team', 'will', 'win', 'the', 'world', 'cup', ',', 'says', 'Virat', 'Kohli', ',', 'since', 'world', 'cup', 'happens', 'in', 'India'], ['We', 'will', 'win', 'the', 'election', 'this', 'year', ',', 'says', 'Stalin'], ['Rabindranath', 'Tagore', 'won', 'the', 'hearts', 'of', 'people'], ['Raasi', 'is', 'an', 'exciting', 'thriller', 'movie']]\n"
     ]
    }
   ],
   "source": [
    "# convert word into numerical features\n",
    "# build common vocabulary and to vectorise the document \n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = [\"Indian cricket team will win the world cup , says Virat Kohli , since world cup happens in India\",\n",
    "    \"We will win the election this year , says Stalin\",\n",
    "          \"Rabindranath Tagore won the hearts of people\" ,\n",
    "          \"Raasi is an exciting thriller movie\",\n",
    "         ]\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "# using map() + split()\n",
    "# Tokenizing strings in list of strings\n",
    "res = list(map(str.split, corpus))\n",
    "  \n",
    "# print result\n",
    "print(\"The list after split of strings is : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "intended-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian', 'cricket', 'team', 'will', 'win', 'the', 'world', 'cup', ',', 'says', 'Virat', 'Kohli', ',', 'since', 'world', 'cup', 'happens', 'in', 'India']\n"
     ]
    }
   ],
   "source": [
    "print(res[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "civilian-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterh = filter_words(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "damaged-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indian', 'cricket', 'team', 'win', 'world', 'cup', ',', 'says', 'Virat', 'Kohli', ',', 'since', 'world', 'cup', 'happens', 'India']\n"
     ]
    }
   ],
   "source": [
    "print(filterh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-dakota",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
